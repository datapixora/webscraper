# ğŸ•·ï¸ WebScraper Platform

A production-grade, full-stack web scraping platform with a modern admin dashboard, powerful API, and intelligent job scheduling.

## ğŸ¯ Features

### Core Capabilities
- **Intelligent Scraping Engine**: Support for both JavaScript-heavy sites (Playwright) and fast HTML parsing
- **Multi-Tenant Architecture**: Isolated projects and data for multiple users
- **Flexible Scheduling**: One-time, hourly, daily, weekly scraping jobs with cron support
- **Rich Extraction Schemas**: CSS selectors, XPath, JSONPath for structured data extraction
- **Real-Time Monitoring**: Track job status, success rates, and performance metrics
- **Webhook Support**: Push notifications on job completion
- **Proxy Rotation**: Built-in proxy management for rate limiting and IP rotation
- **Result Export**: JSON, CSV formats with API access

### Admin Dashboard
- ğŸ“Š **Dashboard Overview**: KPIs, charts, and real-time statistics
- ğŸ—‚ï¸ **Project Management**: Create, edit, and manage scraping projects
- âš™ï¸ **Job Control**: Manual triggers, status monitoring, log inspection
- ğŸ“‹ **Results Viewer**: Paginated data tables with JSON viewer and CSV export
- ğŸ” **User Management**: Role-based access control (admin, client, viewer)
- ğŸ› ï¸ **System Settings**: Proxy configuration, rate limits, API keys

### API
- **RESTful API**: Comprehensive endpoints for all operations
- **JWT Authentication**: Secure token-based auth
- **Rate Limiting**: Configurable per-user limits
- **Pagination**: Efficient data retrieval
- **OpenAPI/Swagger**: Auto-generated interactive documentation

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js UI    â”‚â”€â”€â”€â”€â”€â–¶â”‚  FastAPI API    â”‚â”€â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â”‚   (Dashboard)   â”‚      â”‚   (Backend)     â”‚      â”‚   (Database)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Celery Worker  â”‚â—€â”€â”€â”€â”€â”€â”‚      Redis      â”‚
                         â”‚  (Scraper Jobs) â”‚      â”‚  (Queue/Cache)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Playwright    â”‚
                         â”‚   (Browser)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.11+** with **FastAPI**
- **PostgreSQL** for data persistence
- **Redis** for caching and task queue
- **Celery** for background job processing
- **Playwright** for browser automation
- **SQLAlchemy** + **Alembic** for ORM and migrations

### Frontend
- **Next.js 14** (App Router) with **React 18**
- **TypeScript** for type safety
- **Tailwind CSS** for styling
- **TanStack Query** for data fetching
- **Zustand** for state management
- **Recharts** for data visualization

### DevOps
- **Docker** + **Docker Compose** for containerization
- **Flower** for Celery monitoring
- **Structured logging** (JSON) with OpenTelemetry-ready format

---

## ğŸš€ Quick Start

### Prerequisites
- **Docker** and **Docker Compose** installed
- **Make** (optional, for convenient commands)
- **Git**

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/webscraper.git
   cd webscraper
   ```

2. **Create environment file**
   ```bash
   make create-env
   # OR
   cp .env.example .env
   ```

3. **Update environment variables**
   Edit `.env` and set required values:
   - `SECRET_KEY`: Generate with `openssl rand -hex 32`
   - `POSTGRES_PASSWORD`: Set a strong password
   - Update `CORS_ORIGINS` if needed

4. **Initialize and start the project**
   ```bash
   make init
   ```

   This will:
   - Build Docker images
   - Start all services
   - Run database migrations
   - Seed initial data

5. **Access the services**
   - **Frontend Dashboard**: http://localhost:3000
   - **Backend API**: http://localhost:8000
   - **API Documentation**: http://localhost:8000/docs
   - **Flower (Celery Monitor)**: http://localhost:5555

---

## ğŸ“– Usage

### Using Make Commands

```bash
# Development
make dev              # Start development environment
make logs             # View all logs
make logs-backend     # View backend logs only
make logs-worker      # View worker logs only

# Database
make migrate          # Run migrations
make migrate-create MESSAGE="your migration"  # Create new migration
make db-reset         # Reset database (WARNING: destructive)
make seed             # Seed sample data

# Testing
make test             # Run all tests
make test-backend     # Run backend tests
make test-backend-cov # Run backend tests with coverage

# Code Quality
make format           # Format code (Python & TypeScript)
make lint             # Lint code
make lint-fix         # Fix linting issues

# Shell Access
make backend-shell    # Access backend container
make db-shell         # Access PostgreSQL
make redis-shell      # Access Redis CLI

# Utilities
make playwright-install  # Install Playwright browsers
make backup-db        # Backup database
make api-docs         # Open API documentation
```

### Manual Docker Commands

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Rebuild images
docker-compose build

# Run migrations
docker-compose exec backend alembic upgrade head
```

---

## ğŸ“ Project Structure

```
webscraper/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # Core configuration
â”‚   â”‚   â”œâ”€â”€ models/         # Database models
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”‚   â”œâ”€â”€ repositories/   # Data access layer
â”‚   â”‚   â”œâ”€â”€ scraper/        # Scraping engine
â”‚   â”‚   â”œâ”€â”€ workers/        # Celery tasks
â”‚   â”‚   â””â”€â”€ db/            # Database & migrations
â”‚   â”œâ”€â”€ tests/             # Test suite
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â”‚
â”œâ”€â”€ frontend/               # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # App Router pages
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ lib/           # Utilities
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom hooks
â”‚   â”‚   â”œâ”€â”€ types/         # TypeScript types
â”‚   â”‚   â””â”€â”€ contexts/      # React contexts
â”‚   â””â”€â”€ package.json       # Node dependencies
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ docker-compose.yml      # Development orchestration
â””â”€â”€ Makefile               # Convenience commands
```

---

## ğŸ” Security Considerations

### Best Practices Implemented
- âœ… **JWT-based authentication** with secure token handling
- âœ… **Password hashing** using bcrypt
- âœ… **Multi-tenant data isolation** at database level
- âœ… **CORS configuration** for API security
- âœ… **Rate limiting** to prevent abuse
- âœ… **Environment-based secrets** (never hardcoded)
- âœ… **robots.txt compliance** (configurable per project)
- âœ… **Request throttling** to avoid DoS on target sites

### Configuration
- Always use strong, unique `SECRET_KEY` in production
- Enable HTTPS in production environments
- Rotate API keys regularly
- Configure proxy settings to avoid IP bans
- Review and respect target website ToS and rate limits

---

## ğŸ§ª Testing

### Backend Tests
```bash
# Run all backend tests
make test-backend

# Run with coverage report
make test-backend-cov

# Run specific test file
docker-compose exec backend pytest tests/unit/test_scraper.py -v
```

### Frontend Tests
```bash
# Run all frontend tests
make test-frontend

# Run in watch mode
docker-compose exec frontend npm run test:watch

# Run E2E tests
make test-frontend-e2e
```

---

## ğŸ“Š API Documentation

Once the backend is running, access interactive API documentation:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Key Endpoints

#### Authentication
- `POST /api/v1/auth/login` - Login and get JWT token
- `POST /api/v1/auth/register` - Register new user

#### Projects
- `GET /api/v1/projects` - List all projects
- `POST /api/v1/projects` - Create new project
- `GET /api/v1/projects/{id}` - Get project details
- `PATCH /api/v1/projects/{id}` - Update project
- `DELETE /api/v1/projects/{id}` - Delete project

#### Jobs
- `GET /api/v1/jobs` - List jobs (with filters)
- `POST /api/v1/projects/{id}/jobs` - Trigger manual job
- `GET /api/v1/jobs/{id}` - Get job details

#### Results
- `GET /api/v1/jobs/{id}/results` - Get job results
- `GET /api/v1/projects/{id}/results` - Get all project results

---

## ğŸ”§ Configuration

### Environment Variables

Key environment variables (see `.env.example` for full list):

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | `postgresql://...` |
| `REDIS_URL` | Redis connection string | `redis://localhost:6379/0` |
| `SECRET_KEY` | JWT secret key | **Must set in production** |
| `CORS_ORIGINS` | Allowed CORS origins | `http://localhost:3000` |
| `PLAYWRIGHT_BROWSER` | Browser type | `chromium` |
| `MAX_CONCURRENT_JOBS` | Max parallel scraping jobs | `5` |
| `DEFAULT_REQUEST_DELAY` | Delay between requests (ms) | `1000` |
| `PROXY_ENABLED` | Enable proxy rotation | `false` |

---

## ğŸ“ˆ Monitoring

### Celery Monitoring with Flower
Access Flower at http://localhost:5555 to monitor:
- Active workers
- Task success/failure rates
- Queue length
- Task runtime statistics

### Logs
Structured JSON logs are written to `./logs/app.log` and stdout.

```bash
# View real-time logs
make logs

# Filter backend logs
make logs-backend

# Filter worker logs
make logs-worker
```

---

## ğŸš€ Deployment

### Production Checklist

1. **Update environment variables**
   - Set strong `SECRET_KEY`
   - Use production database credentials
   - Configure proper `CORS_ORIGINS`
   - Enable HTTPS

2. **Build production images**
   ```bash
   make build-prod
   ```

3. **Run database migrations**
   ```bash
   docker-compose -f docker-compose.prod.yml exec backend alembic upgrade head
   ```

4. **Start production services**
   ```bash
   make up-prod
   ```

### Recommended Infrastructure
- **Containerization**: Deploy with Kubernetes or managed container services (AWS ECS, Google Cloud Run)
- **Database**: Managed PostgreSQL (AWS RDS, Google Cloud SQL, Azure Database)
- **Cache**: Managed Redis (AWS ElastiCache, Redis Cloud)
- **CDN**: CloudFlare, AWS CloudFront for static assets
- **Monitoring**: Sentry for error tracking, Datadog/New Relic for APM

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style
- Backend: Follow PEP 8, use `black` and `isort`
- Frontend: Use Prettier with default settings
- Run `make format` and `make lint` before committing

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- FastAPI for the excellent web framework
- Playwright for reliable browser automation
- Next.js for the modern React framework
- The open-source community

---

## ğŸ“ Support

- **Issues**: https://github.com/yourusername/webscraper/issues
- **Discussions**: https://github.com/yourusername/webscraper/discussions
- **Email**: support@webscraper.com

---

**Built with â¤ï¸ for the web scraping community**
#   w e b s c r a p e r  
 #   w e b s c r a p e r  
 